{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 2\n",
    "Frederik Kallestrup Mastratis (qln174)  \n",
    "Dongyu Liu (dlf327)  \n",
    "Shamim Tariq Akram (zmx145)  \n",
    "Celina Aurora Nguyen (szf345)  \n",
    "\n",
    "Group 21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "\n",
    "\n",
    "A(_id_, domain, type, url, content, timestamps, title, summary)\n",
    "\n",
    "T(_id_:int, _tagid_:string)\n",
    "\n",
    "AU(_id_:int, _name_:string)\n",
    "\n",
    "K(_id_, _keyword_:string)\n",
    "\n",
    "MK(_id_, _keyword_:string)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "\n",
    "df_load = pd.read_csv(\"1mio-raw.csv\", delimiter = \",\", nrows = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyalong(df, column, f):\n",
    "    df[column] = df[column].apply(f)\n",
    "\n",
    "# Splits a string into list\n",
    "def string_splitter(string):\n",
    "    lst = str(string).split(\", \")\n",
    "    filter_obj = filter(lambda x: x != \"\", lst)\n",
    "    return list(filter_obj)\n",
    "\n",
    "# Strip a string representation of list of strings\n",
    "def string_stripper(string):\n",
    "    lst = [i.strip() for i in string[1:-1].replace('\\'',\"\").split(',')]\n",
    "    filter_obj = filter(lambda x: x != \"\", lst)\n",
    "    return list(filter_obj)\n",
    "\n",
    "def string_filter(lst):\n",
    "    filters = [lambda x: not x.isdigit(), lambda x: x != \"\"]\n",
    "    filter_obj = filter(lambda x: all([f(x) for f in filters]), lst)\n",
    "    return list(filter_obj)\n",
    "\n",
    "df = df_load.copy()\n",
    "\n",
    "# Dropping columns (setting new ID column later)\n",
    "df = df.drop(columns = ['Unnamed: 0', 'id', 'source'])\n",
    "\n",
    "# Set new ID column\n",
    "df = df.rename_axis('id').reset_index()\n",
    "df.set_index('id')\n",
    "\n",
    "df = df.astype({'domain':str, 'type':str, 'url':str, 'content':str, 'scraped_at':str, 'inserted_at':str,\n",
    "        'updated_at':str, 'title':str, 'authors':str, 'keywords':str, 'meta_keywords':str,\n",
    "        'meta_description':str, 'tags':str, 'summary':str}, copy = False)\n",
    "\n",
    "# Convert blank fields into NaN\n",
    "#df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "\n",
    "# Replace 'nan' strings with NaN\n",
    "df = df.replace(\"nan\", np.nan)\n",
    "\n",
    "# Convert all strings into lower case:\n",
    "df = df.applymap(lambda s: s.lower() if type(s) == str else s)\n",
    "\n",
    "# Clean types\n",
    "type_set = ['fake', 'satire', 'bias', 'conspiracy', 'state', 'junksci', 'hate', 'clickbait', 'unreliable', 'political', 'reliable','rumor']\n",
    "df['type'] = df['type'].apply(lambda x: np.nan if x not in type_set else x)\n",
    "\n",
    "\n",
    "\n",
    "# Clean timestamps\n",
    "#map(lambda t: applyalong(df, t, lambda x: pd.to_datetime(x, errors='coerce')), ['scraped_at','inserted_at','updated_at'])\n",
    "for column in ['scraped_at','inserted_at','updated_at']:\n",
    "    df[column] = df[column].apply(lambda x: pd.to_datetime(x, errors='coerce'))\n",
    "\n",
    "# Clean auhtors - separate into list of strings\n",
    "df['authors'] = df['authors'].apply(lambda x: string_splitter(x) if pd.notnull(x) else x)\n",
    "\n",
    "# Clean metakeywords - strip a string representation of list of strings\n",
    "df['meta_keywords'] = df['meta_keywords'].apply(string_stripper)\n",
    "#df['meta_keywords'] = df['meta_keywords'].apply(lambda x: np.nan if (isinstance(x, list) and len(x)==0) else x)\n",
    "\n",
    "# Clean tags\n",
    "df['tags'] = df['tags'].apply(lambda x: string_splitter(x) if pd.notnull(x) else x)\n",
    "df['tags'] = df['tags'].apply(lambda x: string_filter(x) if isinstance(x, list) else x)\n",
    "#df['tags'] = df['tags'].apply(lambda x: np.nan if (isinstance(x, list) and len(x)==0) else x)\n",
    "\n",
    "# Replace NaN into empty lists\n",
    "for column in ['authors', 'keywords', 'meta_keywords', 'tags']:\n",
    "    df[column] = df[column].fillna(\"\").apply(list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df[\"authors\"]))\n",
    "df['scraped_at'].to_csv('test.csv', index=False)\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = [\"artikel\", \"author\", \"tags\", \"keywords\", \"metakeywords\"]\n",
    "\n",
    "create_tables_all = [\n",
    "    \"\"\" \n",
    "    CREATE TABLE artikel (\n",
    "        id INT, \n",
    "        domain VARCHAR, \n",
    "        type VARCHAR, \n",
    "        url VARCHAR, \n",
    "        content VARCHAR, \n",
    "        scraped_at TIMESTAMP, \n",
    "        inserted_at TIMESTAMP,\n",
    "        updated_at TIMESTAMP, \n",
    "        title VARCHAR (256), \n",
    "        meta_description VARCHAR, \n",
    "        summary VARCHAR,\n",
    "\n",
    "        PRIMARY KEY (id) \n",
    "    );\n",
    "    \"\"\"\n",
    "    ,\n",
    "    \"\"\"\n",
    "    CREATE TABLE author (\n",
    "        a_id INT,\n",
    "        author VARCHAR,\n",
    "        PRIMARY KEY (a_id, author),\n",
    "        FOREIGN KEY (a_id)\n",
    "            REFERENCES artikel (id)\n",
    "            ON UPDATE CASCADE ON DELETE CASCADE\n",
    "    );\n",
    "    \"\"\"\n",
    "    ,\n",
    "    \"\"\" \n",
    "    CREATE TABLE tags (\n",
    "        a_id INT, \n",
    "        tag VARCHAR, \n",
    "        PRIMARY KEY (a_id, tag),\n",
    "        FOREIGN KEY (a_id)\n",
    "            REFERENCES artikel (id)\n",
    "            ON UPDATE CASCADE ON DELETE CASCADE\n",
    "    );\n",
    "    \"\"\"\n",
    "    ,\n",
    "    \"\"\"\n",
    "    CREATE TABLE keywords (\n",
    "        a_id INT, \n",
    "        keyword VARCHAR,\n",
    "        PRIMARY KEY (a_id, keyword),\n",
    "        FOREIGN KEY (a_id)\n",
    "            REFERENCES artikel (id)\n",
    "            ON UPDATE CASCADE ON DELETE CASCADE\n",
    "    );\n",
    "    \"\"\"\n",
    "    ,\n",
    "    \"\"\"\n",
    "    CREATE TABLE metakeywords (\n",
    "        a_id INT, \n",
    "        mkeyword VARCHAR,\n",
    "        PRIMARY KEY (a_id, mkeyword),\n",
    "        FOREIGN KEY (a_id)\n",
    "            REFERENCES artikel (id)\n",
    "            ON UPDATE CASCADE ON DELETE CASCADE\n",
    "    );\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "conn = psycopg2.connect(dbname=\"fakenewsdb\", user=\"postgres\", password=\"1234\")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "for table in tables:\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS \" + table + \" CASCADE;\")\n",
    "\n",
    "for sql in create_tables_all:\n",
    "    cursor.execute(sql)\n",
    "\n",
    "conn.commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projectrow2tuple(fields, row):\n",
    "    return tuple(map(lambda f: row[f], fields))\n",
    "\n",
    "def insertstring(table, n):\n",
    "    return \"INSERT INTO {} VALUES ({}) ON CONFLICT DO NOTHING\".format(table, \", \".join(map(lambda _: '%s', range(n))))\n",
    "\n",
    "def multi_insert(server, a_id, insert, xs):\n",
    "    for x in xs:\n",
    "        server.execute(insert, (a_id, x))\n",
    "    \n",
    "\n",
    "def insert_csv2rows(server, csv_row):\n",
    "    A_domain = ['id', 'domain', 'type', 'url', 'content', 'scraped_at',\n",
    "       'inserted_at', 'updated_at', 'title', 'meta_description', 'summary']\n",
    "    AU_domain = ['id', 'authors']\n",
    "    T_domain = ['id', 'tags']\n",
    "    K_domain = ['id', 'keywords']\n",
    "    MK_domain = ['id', 'meta_keywords']\n",
    "    \n",
    "    Atuple = projectrow2tuple(A_domain, csv_row)\n",
    "    (a_id, tags) = projectrow2tuple(T_domain, csv_row)\n",
    "    (_, au) = projectrow2tuple(AU_domain, csv_row)\n",
    "    (_, kws) = projectrow2tuple(K_domain, csv_row)\n",
    "    (_, mkws) = projectrow2tuple(MK_domain, csv_row)\n",
    "\n",
    "    Ainsert = insertstring(\"artikel\", len(Atuple))\n",
    "    Tinsert = insertstring(\"tags\", 2)\n",
    "    AUinsert = insertstring(\"author\", 2)\n",
    "    Kinsert = insertstring(\"keywords\", 2)\n",
    "    MKinsert = insertstring(\"metakeywords\", 2)\n",
    "\n",
    "    server.execute(Ainsert, Atuple)\n",
    "    insert = lambda ins, xs: multi_insert(server, a_id, ins, xs)\n",
    "    insert(Tinsert, tags)\n",
    "    insert(AUinsert, au)\n",
    "    insert(Kinsert, kws)\n",
    "    insert(MKinsert, mkws)\n",
    "\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    insert_csv2rows(cursor, row)\n",
    "\n",
    "conn.commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"SELECT * FROM author;\")\n",
    "cursor.fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUERY 1\n",
    "\n",
    "query1 = \"\"\"\n",
    "    SELECT distinct domain, type, scraped_at \n",
    "    FROM artikel\n",
    "    WHERE type = 'reliable' AND scraped_at >= '2018-01-15'\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(query1)\n",
    "cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUERY 2\n",
    "\n",
    "query2 = \"\"\"\n",
    "    SELECT author\n",
    "    FROM (\n",
    "        SELECT COUNT(a_id) AS cnt, author \n",
    "        FROM author \n",
    "        WHERE a_id IN (\n",
    "            SELECT id\n",
    "            FROM artikel\n",
    "            WHERE type = 'fake'\n",
    "        ) \n",
    "        GROUP BY author\n",
    "    ) AS x\n",
    "    WHERE cnt = (SELECT MAX(cnt) FROM x)\n",
    "\"\"\"\n",
    "\n",
    "# This gives a max, but doesnt take into account if multiple authors write max number of fake articles\n",
    "query2_pseudomax = \"\"\"\n",
    "    SELECT COUNT(a_id), author \n",
    "    FROM author \n",
    "    WHERE a_id IN (\n",
    "        SELECT id\n",
    "        FROM artikel\n",
    "        WHERE type = 'fake'\n",
    "    )\n",
    "    GROUP BY author ORDER BY COUNT(a_id) DESC LIMIT 1\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute('rollback')\n",
    "cursor.execute(query2_pseudomax)\n",
    "cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
