{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 2\n",
    "Frederik Kallestrup Mastratis (qln174)  \n",
    "Dongyu Liu (dlf327)  \n",
    "Shamim Tariq Akram (zmx145)  \n",
    "Celina Aurora Nguyen (szf345)  \n",
    "\n",
    "Group 21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "\n",
    "\n",
    "A(_id_, domain, type, url, content, timestamps, title, summary)\n",
    "\n",
    "T(_id_:int, _tagid_:string)\n",
    "\n",
    "AU(_id_:int, _name_:string)\n",
    "\n",
    "K(_id_, _keyword_:string)\n",
    "\n",
    "MK(_id_, _keyword_:string)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "\n",
    "df_load = pd.read_csv(\"1mio-raw.csv\", delimiter = \",\", nrows = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyalong(df, column, f):\n",
    "    df[column] = df[column].apply(f)\n",
    "\n",
    "# Splits a string into list\n",
    "def string_splitter(string):\n",
    "    lst = str(string).split(\", \")\n",
    "    filter_obj = filter(lambda x: x != \"\", lst)\n",
    "    return list(filter_obj)\n",
    "\n",
    "# Strip a string representation of list of strings\n",
    "def string_stripper(string):\n",
    "    lst = [i.strip() for i in string[1:-1].replace('\\'',\"\").split(',')]\n",
    "    filter_obj = filter(lambda x: x != \"\", lst)\n",
    "    return list(filter_obj)\n",
    "\n",
    "def string_filter(lst):\n",
    "    filters = [lambda x: not x.isdigit(), lambda x: x != \"\"]\n",
    "    filter_obj = filter(lambda x: all([f(x) for f in filters]), lst)\n",
    "    return list(filter_obj)\n",
    "\n",
    "df = df_load.copy()\n",
    "\n",
    "# Dropping columns (setting new ID column later)\n",
    "df = df.drop(columns = ['Unnamed: 0', 'id', 'source'])\n",
    "\n",
    "# Set new ID column\n",
    "df = df.rename_axis('id').reset_index()\n",
    "\n",
    "df = df.astype({'domain':str, 'type':str, 'url':str, 'content':str, 'scraped_at':str, 'inserted_at':str,\n",
    "        'updated_at':str, 'title':str, 'authors':str, 'keywords':str, 'meta_keywords':str,\n",
    "        'meta_description':str, 'tags':str, 'summary':str}, copy = False)\n",
    "\n",
    "# Convert blank fields into NaN\n",
    "df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "\n",
    "# Replace 'nan' strings with NaN\n",
    "df = df.replace(\"nan\", np.nan)\n",
    "\n",
    "# Convert all strings into lower case:\n",
    "df = df.applymap(lambda s: s.lower() if type(s) == str else s)\n",
    "\n",
    "# Clean types\n",
    "type_set = ['fake', 'satire', 'bias', 'conspiracy', 'state', 'junksci', 'hate', 'clickbait', 'unreliable', 'political', 'reliable','rumor']\n",
    "df['type'] = df['type'].apply(lambda x: np.nan if x not in type_set else x)\n",
    "\n",
    "# Clean timestamps\n",
    "#map(lambda t: applyalong(df, t, lambda x: pd.to_datetime(x, errors='coerce')), ['scraped_at','inserted_at','updated_at'])\n",
    "for column in ['scraped_at','inserted_at','updated_at']:\n",
    "    df[column] = df[column].apply(lambda x: pd.to_datetime(x, errors='coerce'))\n",
    "\n",
    "# Clean auhtors - separate into list of strings\n",
    "df['authors'] = df['authors'].apply(lambda x: string_splitter(x) if pd.notnull(x) else x)\n",
    "\n",
    "# Clean metakeywords - strip a string representation of list of strings\n",
    "df['meta_keywords'] = df['meta_keywords'].apply(string_stripper)\n",
    "df['meta_keywords'] = df['meta_keywords'].apply(lambda x: np.nan if (isinstance(x, list) and len(x)==0) else x)\n",
    "\n",
    "# Clean tags\n",
    "df['tags'] = df['tags'].apply(lambda x: string_splitter(x) if pd.notnull(x) else x)\n",
    "df['tags'] = df['tags'].apply(lambda x: string_filter(x) if isinstance(x, list) else x)\n",
    "df['tags'] = df['tags'].apply(lambda x: np.nan if (isinstance(x, list) and len(x)==0) else x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df[\"authors\"]))\n",
    "df['tags'].to_csv('test.csv')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_main = \"\"\"\n",
    "    CREATE TABLE main (\n",
    "        id SERIAL,\n",
    "        domain VARCHAR,\n",
    "        type VARCHAR,\n",
    "        url VARCHAR,\n",
    "        content VARCHAR,\n",
    "        scraped_at TIMESTAMP,\n",
    "        inserted_at TIMESTAMP,\n",
    "        updated_at TIMESTAMP,\n",
    "        title VARCHAR,\n",
    "        authors VARCHAR[],\n",
    "        keywords VARCHAR[],\n",
    "        meta_keywords VARCHAR[],\n",
    "        meta_description VARCHAR,\n",
    "        tags VARCHAR[],\n",
    "        summary VARCHAR\n",
    "    );\n",
    "    \"\"\"\n",
    "\n",
    "# FOR TAs: To connect to own server 'postgresql://user:password@host/database'\n",
    "conn_string = 'postgresql://postgres:1234@127.0.0.1/fakenewsdb'\n",
    "\n",
    "\n",
    "conn = psycopg2.connect(dbname=\"fakenewsdb\", user=\"postgres\", password=\"1234\")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(create_main)\n",
    "\n",
    "with open('1mio-raw.csv', 'r') as f:\n",
    "    next(f) # Skip the header row.\n",
    "    cursor.copy_from(f, 'main', sep=',')\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "conn.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "conn = psycopg2.connect(conn_string)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(create_main)\n",
    "\n",
    "db = create_engine(conn_string)\n",
    "conn = db.connect()\n",
    "  \n",
    "# Create DataFrame\n",
    "df.to_sql('main', con=conn, if_exists='replace',\n",
    "          index=False)\n",
    "\n",
    "conn = psycopg2.connect(conn_string)\n",
    "\n",
    "conn.autocommit = True\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "\n",
    "  \n",
    "#sql1 = '''select * from data;'''\n",
    "#cursor.execute(sql1)\n",
    "#for i in cursor.fetchall():\n",
    "#    print(i)\n",
    "  \n",
    "# conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "conn = psycopg2.connect(dbname=\"fakenewsdb\", user=\"postgres\", password=\"1234\")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(create_main)\n",
    "\n",
    "with open('1mio-raw.csv', 'r') as f:\n",
    "    next(f) # Skip the header row.\n",
    "    cursor.copy_from(f, 'main', sep=',')\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "conn.close()\n",
    "\n",
    "\n",
    "#Read CSV\n",
    "\n",
    "\n",
    "#data = pd.read_csv(\"1mio-raw.csv\", delimiter=\",\", nrows=100)\n",
    "#print(data.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = [\"artikel\", \"author\", \"tags\", \"keywords\", \"metakeywords\"]\n",
    "\n",
    "create_tables_all = [\n",
    "    \"\"\" \n",
    "    CREATE TABLE artikel (\n",
    "        id INT, \n",
    "        domain VARCHAR(50), \n",
    "        type VARCHAR(20), \n",
    "        url VARCHAR, \n",
    "        content VARCHAR, \n",
    "        scraped_at TIMESTAMP, \n",
    "        inserted_at TIMESTAMP,\n",
    "        updated_at TIMESTAMP, \n",
    "        title VARCHAR (256), \n",
    "        meta_description VARCHAR, \n",
    "        summary VARCHAR, \n",
    "        PRIMARY KEY (id) \n",
    "    );\n",
    "    \"\"\"\n",
    "    ,\n",
    "    \"\"\"\n",
    "    CREATE TABLE author (\n",
    "        a_id INT,\n",
    "        author VARCHAR(50),\n",
    "        PRIMARY KEY (a_id, author),\n",
    "        FOREIGN KEY (a_id)\n",
    "            REFERENCES artikel (id)\n",
    "            ON UPDATE CASCADE ON DELETE CASCADE\n",
    "    );\n",
    "    \"\"\"\n",
    "    ,\n",
    "    \"\"\" \n",
    "    CREATE TABLE tags (\n",
    "        a_id INT, \n",
    "        tag VARCHAR(50), \n",
    "        PRIMARY KEY (a_id, tag),\n",
    "        FOREIGN KEY (a_id)\n",
    "            REFERENCES artikel (id)\n",
    "            ON UPDATE CASCADE ON DELETE CASCADE\n",
    "    );\n",
    "    \"\"\"\n",
    "    ,\n",
    "    \"\"\"\n",
    "    CREATE TABLE keywords (\n",
    "        a_id INT, \n",
    "        keyword VARCHAR(50),\n",
    "        PRIMARY KEY (a_id, keyword),\n",
    "        FOREIGN KEY (a_id)\n",
    "            REFERENCES artikel (id)\n",
    "            ON UPDATE CASCADE ON DELETE CASCADE\n",
    "    );\n",
    "    \"\"\"\n",
    "    ,\n",
    "    \"\"\"\n",
    "    CREATE TABLE metakeywords (\n",
    "        a_id INT, \n",
    "        mkeyword VARCHAR(50),\n",
    "        PRIMARY KEY (a_id, mkeyword),\n",
    "        FOREIGN KEY (a_id)\n",
    "            REFERENCES artikel (id)\n",
    "            ON UPDATE CASCADE ON DELETE CASCADE\n",
    "    );\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "for table in tables:\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS \" + table + \" CASCADE;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def projectrow2tuple(fields, row):\n",
    "    return tuple(map(lambda f: row[f], fields))\n",
    "\n",
    "def insertstring(table, n):\n",
    "    return \"INSERT INTO {} VALUES ({})\".format(table, \", \".join(map(lambda _: '%s', range(n))))\n",
    "    \n",
    "\n",
    "def insert_csv2rows(csv_row):\n",
    "    A_domain = ['id', 'domain', 'type', 'url', 'content', 'scraped_at',\n",
    "       'inserted_at', 'updated_at', 'title', 'meta_description', 'tags', 'summary', 'source']\n",
    "    AU_domain = ['id', 'author']\n",
    "    T_domain = ['id', 'tags']\n",
    "    K_domain = ['id', 'keywords']\n",
    "    MK_domain = ['id', 'meta_keywords']\n",
    "    \n",
    "    Atuple = projectrow2tuple(A_domain, csv_row)\n",
    "    (id, tags) = projectrow2tuple(T_domain, csv_row)\n",
    "    (_, au) = projectrow2tuple(AU_domain, csv_row)\n",
    "    (_, kws) = projectrow2tuple(K_domain, csv_row)\n",
    "    (_, mkws) = projectrow2tuple(MK_domain, csv_row)\n",
    "\n",
    "    Ainsert = insertstring(\"A\", len(Atuple))\n",
    "    Tinsert = insertstring(\"T\", 2)\n",
    "    AUinsert = insertstring(\"AU\", 2)\n",
    "    Kinsert = insertstring(\"K\", 2)\n",
    "    MKinsert = insertstring(\"MK\", 2)\n",
    "\n",
    "#For each line\n",
    "    #Insert appropriate rows in DB\n",
    "\n",
    "CONTENT?\n",
    "CSV ind i tabel\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#qurrey the database \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(['id', 'domain', 'type', 'url', 'content', 'scraped_at',\n",
    "       'inserted_at', 'updated_at', 'title', 'meta_description', 'tags', 'summary', 'source'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
