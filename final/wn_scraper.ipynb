{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WIKINEWS SCRAPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import datefinder\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_soup(url):\n",
    "    site = requests.get(url)\n",
    "    return bs(site.text, \"html.parser\")\n",
    "\n",
    "soup = create_soup('https://en.wikinews.org/wiki/Category:Politics_and_conflicts')\n",
    "\n",
    "assigned_letters = \"ABCDEFGHIJKLMNOPRSTUVWZABCDEFGHIJKLMNOPRSTUVWZ\"[21%23:21%23+10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_links(soup):\n",
    "    links=[]\n",
    "    def is_valid(tag):\n",
    "        return tag.name == 'a' and tag.get('title')[0] in assigned_letters\n",
    "    section = {'id':\"mw-pages\"}\n",
    "    subsection = {'class':\"mw-category mw-category-columns\"}\n",
    "    for link in soup.find('div', section).find('div', subsection).find_all(is_valid):\n",
    "        links.append(\"https://en.wikinews.org\" + link.get('href'))\n",
    "    return links\n",
    "\n",
    "def next_page_url(soup):\n",
    "    return \"https://en.wikinews.org\" + soup.find('a', string=\"next page\").get('href')\n",
    "    \n",
    "def no_more_pages(soup):\n",
    "    return soup.find('a', string=\"next page\") is None\n",
    "\n",
    "def get_links(soup, links):\n",
    "    links += valid_links(soup)\n",
    "\n",
    "    if no_more_pages(soup):\n",
    "        return links\n",
    "    else:\n",
    "        next_soup = create_soup(next_page_url(soup))\n",
    "        return get_links(next_soup, links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = get_links(soup, [])\n",
    "len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_parser(soup):\n",
    "    publish_date = \"\"\n",
    "    content = \"\"\n",
    "\n",
    "    terminators = ('Have an opinion on this story', 'Share this', 'This article has passed through community review',\n",
    "        'This page is archived')\n",
    "\n",
    "    for tag in soup.find('div', {'id':\"bodyContent\"}).find('div', {'class':\"mw-parser-output\"}).find_all(['p','dl']):\n",
    "        datetag = tag.find('strong', {'class': \"published\"})\n",
    "        \n",
    "        if tag.text.startswith(terminators):\n",
    "            break\n",
    "        elif datetag != None:\n",
    "            publish_date = datetag.span['title']\n",
    "            content += tag.text\n",
    "            content = content[len(datetag.text):] \n",
    "        else:\n",
    "            content += (tag.text + \"\\n\")\n",
    "    \n",
    "    return content, publish_date\n",
    "\n",
    "def footer_parser(soup):\n",
    "    modified_date = \"\"\n",
    "    modified_tag = soup.find('li', {'id':\"footer-info-lastmod\"})\n",
    "    if modified_tag != None:\n",
    "        modified_text = modified_tag.text\n",
    "        dates_from_text = list(datefinder.find_dates(modified_text))\n",
    "        if len(dates_from_text) > 0:\n",
    "            modified_date = dates_from_text[0].strftime('%Y-%m-%d')\n",
    "    \n",
    "    return modified_date\n",
    "\n",
    "def source_parser(soup):\n",
    "    sources = []\n",
    "    for tag in soup.find_all('span', {'class':\"sourceTemplate\"}):\n",
    "        source_text = tag.find('i')\n",
    "        if source_text != None:\n",
    "            sources.append(source_text.text)\n",
    "    \n",
    "    return sources\n",
    "\n",
    "def categories_parser(soup):\n",
    "    categories = []\n",
    "\n",
    "    # Remove common categories\n",
    "    excluded = ['','Published', 'Archived', 'Politics and conflicts']\n",
    "\n",
    "    for tag in soup.find('div', {'id':\"mw-normal-catlinks\"}).find_all('li'):\n",
    "        if tag.text not in excluded:\n",
    "            categories.append(tag.text)\n",
    "    \n",
    "    # Remove date category\n",
    "    if len(list(datefinder.find_dates(categories[0]))) > 0:\n",
    "        categories = categories[1:]\n",
    "\n",
    "    return categories\n",
    "    \n",
    "\n",
    "def scrape(url):\n",
    "    soup = create_soup(url)\n",
    "\n",
    "    title = soup.find('h1', {'id':\"firstHeading\"}).text\n",
    "\n",
    "    content, publish_date = content_parser(soup)\n",
    "\n",
    "    modified_date = footer_parser(soup)\n",
    "\n",
    "    sources = source_parser(soup)\n",
    "\n",
    "    categories = categories_parser(soup)\n",
    "\n",
    "    return title, publish_date, modified_date, sources, categories, content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraped_data = pd.DataFrame(map(scrape, links), columns = ['title', 'publish_date', 'modified_date', 'sources', 'categories', 'content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraped_data.to_csv('wikinews.csv', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
