{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn import linear_model, metrics, naive_bayes, svm, neural_network\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('X_trainFNC.csv').squeeze()\n",
    "X_test = pd.read_csv('X_testFNC.csv').squeeze()\n",
    "X_valid = pd.read_csv('X_validFNC.csv').squeeze()\n",
    "y_train = pd.read_csv('y_trainFNC.csv').squeeze()\n",
    "y_test = pd.read_csv('y_testFNC.csv').squeeze()\n",
    "y_valid = pd.read_csv('y_validFNC.csv').squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorizer = TfidfVectorizer(       \n",
    "    lowercase=False, \n",
    "    ngram_range=(1,1), \n",
    "    token_pattern=r\"(?u)\\b\\w\\w+\\b|<DATE>|<NUM>|<EMAIL>|<URL>\",\n",
    "    min_df=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98724"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(X_train)\n",
    "\n",
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = X_train.shape[0]\n",
    "n_val = X_valid.shape[0]\n",
    "\n",
    "X_trainval = np.concatenate((X_train, X_valid))\n",
    "y_trainval = np.concatenate((y_train, y_valid))\n",
    "\n",
    "test_fold = -1*np.ones(X_trainval.shape[0])\n",
    "test_fold[n_train:] = 0\n",
    "pds = PredefinedSplit(test_fold)\n",
    "\n",
    "TFIDF_Xtrainval = vectorizer.transform(X_trainval)\n",
    "\n",
    "TFIDF_Xtrain = vectorizer.transform(X_train)\n",
    "TFIDF_Xvalid = vectorizer.transform(X_valid)\n",
    "TFIDF_Xtest = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lin Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_model = linear_model.LogisticRegression()\n",
    "\n",
    "LR_param = [\n",
    "    {'penalty' : ['l1'], 'C' : np.logspace(-4, 4, 20), 'solver' : ['liblinear']},\n",
    "    {'penalty' : ['l2'], 'C' : np.logspace(-4, 4, 20), 'solver' : ['lbfgs','newton-cg','saga']}\n",
    "]\n",
    "\n",
    "LR_gs = GridSearchCV(LR_model, param_grid = LR_param, cv=pds, verbose = 5, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_base = LR_model.fit(TFIDF_Xtrain, y_train)\n",
    "\n",
    "LR_base_fn = 'LR_base.sav'\n",
    "\n",
    "pickle.dump(LR_base, open(LR_base_fn, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': [1]}\n"
     ]
    }
   ],
   "source": [
    "LR_gs.fit(TFIDF_Xtrainval, y_trainval)\n",
    "LR_bestparam = LR_gs.best_params_\n",
    "\n",
    "with open('LR_bestparam,txt', 'w') as f:\n",
    "    print(LR_bestparam, file=f)\n",
    "\n",
    "print(LR_bestparam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LR_tuned = linear_model.LogisticRegression(**LR_bestparam)\n",
    "LR_tuned.fit(TFIDF_Xtrain, y_train)\n",
    "\n",
    "LR_tuned_fn = 'LR_tuned.sav'\n",
    "pickle.dump(LR_tuned, open(LR_tuned_fn, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8694711649100197"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load_LR = pickle.load(open('LR_base.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "\n",
    "NB_model = naive_bayes.MultinomialNB()\n",
    "\n",
    "NB_param = [\n",
    "    {'alpha' : [0.0, 0.01, 0.05] + np.linspace(0.1, 1, 10).tolist() + [5.0, 10.0, 100.0]}\n",
    "]\n",
    "\n",
    "NB_gs = GridSearchCV(NB_model, param_grid = NB_param, cv=pds, verbose = 5, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_base = NB_model.fit(TFIDF_Xtrain, y_train)\n",
    "\n",
    "NB_base_fn = 'NB_base.sav'\n",
    "pickle.dump(NB_base, open(NB_base_fn, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_gs.fit(TFIDF_Xtrainval, y_trainval)\n",
    "NB_bestparam = NB_gs.best_params_\n",
    "\n",
    "with open('NB_bestparam,txt', 'w') as f:\n",
    "    print(NB_bestparam, file=f)\n",
    "\n",
    "print(NB_bestparam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_tuned = naive_bayes.MultinomialNB(**NB_bestparam)\n",
    "NB_tuned.fit(TFIDF_Xtrain, y_train)\n",
    "\n",
    "NB_tuned_fn = 'NB_tuned.sav'\n",
    "pickle.dump(NB_tuned, open(NB_tuned_fn, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_model = svm.SVC(probability=True)\n",
    "\n",
    "SVM_param = [\n",
    "    {'kernel' : ['linear'], 'C' : [0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]}\n",
    "]\n",
    "\n",
    "SVM_gs = GridSearchCV(SVM_model, param_grid = SVM_param, cv=pds, verbose = 5, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_base = SVM_model.fit(TFIDF_Xtrain, y_train)\n",
    "\n",
    "SVM_base_fn = 'SVM_base.sav'\n",
    "pickle.dump(SVM_base, open(SVM_base_fn, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_gs.fit(TFIDF_Xtrainval, y_trainval)\n",
    "SVM_bestparam = SVM_gs.best_params_\n",
    "\n",
    "with open('SVM_bestparam,txt', 'w') as f:\n",
    "    print(SVM_bestparam, file=f)\n",
    "\n",
    "print(SVM_bestparam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_tuned = svm.SVC(probability=True, **SVM_bestparam)\n",
    "SVM_tuned.fit(TFIDF_Xtrain, y_train)\n",
    "\n",
    "SVM_tuned_fn = 'SVM_tuned.sav'\n",
    "pickle.dump(SVM_tuned, open(SVM_tuned_fn, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEED FORWARD NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN_model = neural_network.MLPClassifier()\n",
    "\n",
    "ANN_param = {\n",
    "    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)], 'activation': ['logistic', 'relu'], \n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant','adaptive']\n",
    "}\n",
    "\n",
    "ANN_gs = GridSearchCV(ANN_model, param_grid = ANN_param, cv=pds, verbose = 5, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN_base= ANN_model.fit(TFIDF_Xtrain, y_train)\n",
    "\n",
    "ANN_base_fn = 'ANN_base.sav'\n",
    "pickle.dump(ANN_base, open(ANN_base_fn, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN_gs.fit(TFIDF_Xtrainval, y_trainval)\n",
    "ANN_bestparam = ANN_gs.best_params_\n",
    "\n",
    "with open('ANN_bestparam,txt', 'w') as f:\n",
    "    print(ANN_bestparam, file=f)\n",
    "\n",
    "print(ANN_bestparam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN_tuned = neural_network.MLPClassifier(**ANN_bestparam)\n",
    "ANN_tuned.fit(TFIDF_Xtrain, y_train)\n",
    "\n",
    "ANN_tuned_fn = 'ANN_tuned.sav'\n",
    "pickle.dump(ANN_tuned, open(ANN_tuned_fn, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "65cc6a082bbdb51aba06bbf1d78c1b93987957587a078c9335da8e443a5b7d28"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
